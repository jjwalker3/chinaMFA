{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   0,    1,    2,    9,   10,   15,   16,   19,   20,   22,\n",
       "            ...\n",
       "            1513, 1514, 1515, 1516, 1517, 1519, 1523, 1525, 1528, 1531],\n",
       "           dtype='int64', length=471)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "# from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re\n",
    "\n",
    "directory = pd.read_json('directory.json').sort_index()\n",
    "directory.loc[directory['more'].notnull(),'more'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "# from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "directory = pd.read_json('directory.json').sort_index()\n",
    "documents = pd.read_json('documents.json')\n",
    "\n",
    "\n",
    "breaks=[\n",
    "    [0,100],\n",
    "    [100,200],\n",
    "    [200,300],\n",
    "    [300,400],\n",
    "    [400,500],\n",
    "]\n",
    "\n",
    "for z in breaks:\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument('log-level=1')     \n",
    "    driver = webdriver.Chrome(options=options, executable_path=\"chromedriver.exe\")\n",
    "    print(\"Chrome Headless Browser Invoked\")\n",
    "\n",
    "    try:\n",
    "        subdirectory_progress = pd.read_json('subdirectory_progress.json')\n",
    "    except:\n",
    "        subdirectory_progress = pd.DataFrame(columns=['sub_url'])\n",
    "\n",
    "    try:\n",
    "        directory_progress = pd.read_json('directory_progress.json')\n",
    "    except:\n",
    "        directory_progress = pd.DataFrame(columns=['dir_url'])\n",
    "\n",
    "\n",
    "    counter = 0\n",
    "    dir_counter = 0\n",
    "    problems = []\n",
    "\n",
    "\n",
    "\n",
    "    for i in directory.loc[directory['more'].notnull(),'more'].index[z[0]:z[1]]:\n",
    "        try:\n",
    "            url_root = directory.loc[i,'url']\n",
    "            if url_root in set(directory_progress['dir_url']):\n",
    "                print('SKIP DIR')\n",
    "                continue\n",
    "            else:\n",
    "                print('\\n','ROOT')\n",
    "                print(url_root)\n",
    "                print('\\n','SUBS')\n",
    "                country = directory.loc[i,'country']\n",
    "                page = directory.loc[i,'page']\n",
    "                lang = directory.loc[i,'lang']\n",
    "\n",
    "                subpage_count = int(re.split(r'_|\\.',directory.loc[i,'more'])[1])\n",
    "                subpages = np.arange(subpage_count)+1\n",
    "                subpages_urls = [(url_root+'default_'+str(m)+'.shtml') for m in subpages]\n",
    "\n",
    "                dir_counter += 1\n",
    "\n",
    "                for h in subpages_urls:\n",
    "                    if h in set(subdirectory_progress['sub_url']):\n",
    "                        print('SKIP SUB')\n",
    "                        continue\n",
    "                    else:                \n",
    "                        print(h,'\\n')\n",
    "                        driver.get(h)\n",
    "                        soup=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "                        nav = str()\n",
    "                        for k in soup.find(class_='nav').find_all('a'):\n",
    "                            nav = nav + k.text + '_'\n",
    "\n",
    "                #         try:\n",
    "                #             more = soup.find(\"a\", string='尾页')['href']\n",
    "                #             print(more)\n",
    "                #             directory.loc[i,'more'] = more\n",
    "                #         except:\n",
    "                #             print('NO MORE')\n",
    "\n",
    "                        for j in soup.find(class_='rebox_news').find_all('li'):\n",
    "                            title = j.text\n",
    "                            url_new = url_root+j.find('a')['href'][2:]\n",
    "                            print(url_new)\n",
    "\n",
    "                            try:\n",
    "                                date = re.findall(r\"\\d\\d\\d\\d\\-\\d\\d\\-\\d\\d\",j.text)[0]\n",
    "                            except:\n",
    "                                date = '9999-99-99'\n",
    "                            print(date)\n",
    "\n",
    "                            documents = documents.append({'country':country,\n",
    "                                                          'page':page,\n",
    "                                                          'url':url_new,\n",
    "                                                          'lang':lang,\n",
    "                                                          'nav':nav,\n",
    "                                                          'title':title,\n",
    "                                                          'date':date,\n",
    "                                                         },\n",
    "                                                         ignore_index=True)\n",
    "                            subdirectory_progress = subdirectory_progress.append({'sub_url':h},ignore_index=True)\n",
    "                            counter += 1\n",
    "                            print(dir_counter)\n",
    "                            print(counter,'\\n')\n",
    "\n",
    "                    directory_progress = directory_progress.append({'dir_url':url_root},ignore_index=True)\n",
    "        except:\n",
    "            problems.append(i)\n",
    "\n",
    "    documents = documents.drop_duplicates()\n",
    "    subdirectory_progress = subdirectory_progress.drop_duplicates()\n",
    "    directory_progress = directory_progress.drop_duplicates()\n",
    "\n",
    "    documents.to_json('documents.json')\n",
    "    subdirectory_progress.to_json('subdirectory_progress.json')\n",
    "    directory_progress.to_json('directory_progress.json')\n",
    "    pd.Series(problems).to_json('problems.json')\n",
    "    driver.quit()\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
